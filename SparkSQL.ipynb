{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRSf7jQnXpa5hOQKc0IRVZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancadeFrancisco/BigData_SparkSQL/blob/main/SparkSQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBCgwWUyoWBi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae5fc157-1b53-400a-d37c-94598880a1c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285388 sha256=7c812214fbbbe05c30a736b227ab7679f910504a1722a8ccce77753c6ff70951\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark #==3.3.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSPPhdu4ofdN",
        "outputId": "d7b30406-ca59-420a-e22e-21f4b4f2a4c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-22 14:25:37--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.237.133.81, 18.205.222.128, 54.161.241.46, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.237.133.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13921656 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.28M  56.0MB/s    in 0.2s    \n",
            "\n",
            "2023-08-22 14:25:38 (56.0 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13921656/13921656]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# ConfigureSparkUI\n",
        "conf = SparkConf().set('spark.ui.port', '4050')\n",
        "sc = SparkContext(conf=conf)\n",
        "sc.stop()\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder                  # Método da classe que constrói a sessão spark\n",
        "      .appName(\"Meu Primeiro App Spark\")  # Nome do App Spark\n",
        "      .getOrCreate())                     # Verifica se há uma sessão ativa, e se não há, cria uma nova sessão\n"
      ],
      "metadata": {
        "id": "dReuMFxIog0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s http://localhost:4040/api/tunnels"
      ],
      "metadata": {
        "id": "chLboszBogvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SparkSQL"
      ],
      "metadata": {
        "id": "NqP6GEx9onD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, TimestampType\n",
        "\n",
        "caminho_csv = \"./base_de_dados.csv\"\n",
        "\n",
        "schema_base_pix = StructType([\n",
        "    StructField('id', IntegerType()),\n",
        "    StructField('valor', DoubleType()),\n",
        "    StructField('parte_debitada_nome', StringType()),\n",
        "    StructField('parte_debitada_conta', StringType()),\n",
        "    StructField('parte_debitada_banco', StringType()),\n",
        "    StructField('parte_creditada_nome', StringType()),\n",
        "    StructField('parte_creditada_conta', StringType()),\n",
        "    StructField('parte_creditada_banco', StringType()),\n",
        "    StructField('chave_pix_tipo', StringType()),\n",
        "    StructField('chave_pix_valor', StringType()),\n",
        "    StructField('data_transacao', TimestampType())\n",
        "])\n",
        "\n",
        "df = spark.read.csv(\n",
        "    path=caminho_csv,\n",
        "    header=True,\n",
        "    sep=\";\",\n",
        "    schema=schema_base_pix,\n",
        "    timestampFormat=\"dd/MM/yyyy HH:mm\"\n",
        ")\n",
        "spark.read.csv(\n",
        "    path=caminho_csv,\n",
        "    header=True,\n",
        "    sep=\";\",\n",
        "    schema=schema_base_pix,\n",
        "    timestampFormat=\"dd/MM/yyyy HH:mm\"\n",
        ").createOrReplaceTempView(\"base_pix\")   # CRIEI UMA TABELA COM O NOME BASE_PIX"
      ],
      "metadata": {
        "id": "82nMKdHlogo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select * from base_pix limit 10\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3s6Gt3Qogd4",
        "outputId": "f319e288-7979-4b6d-b741-d0663d321965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+--------------+---------------+-------------------+\n",
            "| id|   valor| parte_debitada_nome|parte_debitada_conta|parte_debitada_banco|parte_creditada_nome|parte_creditada_conta|parte_creditada_banco|chave_pix_tipo|chave_pix_valor|     data_transacao|\n",
            "+---+--------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+--------------+---------------+-------------------+\n",
            "|  1|    9.93|Dra. Ana Carolina...|            79470453|              Nubank|       Maysa da Cruz|             67162333|                 Itau|           cpf|     8439752610|2022-02-18 13:28:00|\n",
            "|  2|   15.38|        Ana Caldeira|            19689668|                Itau|        Evelyn Sales|             60005091|             Bradesco|           cpf|    27145380617|2022-04-08 01:47:00|\n",
            "|  3|   57.58|    Arthur Goncalves|            18856899|            Bradesco|          Maria Melo|             13496303|                  BTG|           cpf|    16452937006|2022-07-14 03:18:00|\n",
            "|  4|53705.13|  Ana Julia Caldeira|            22834741|                Itau|   Ana Livia Almeida|             44695116|               Nubank|           cpf|    26590384142|2022-01-15 18:06:00|\n",
            "|  5|25299.69|  Srta. Nicole Pinto|             3715882|              Nubank|Srta. Ana Laura d...|             21409465|               Nubank|           cpf|    73486105280|2022-05-13 11:04:00|\n",
            "|  6| 7165.06|   Gabriela Ferreira|             2243037|              Nubank|       Larissa Souza|             10689552|                 Itau|           cpf|    96845371237|2022-09-11 13:38:00|\n",
            "|  7|    6.16|    Heloisa da Rocha|            59778949|                 BTG|Dra. Vitoria Silv...|             56583792|               Nubank|           cpf|    89064175357|2021-12-10 12:37:00|\n",
            "|  8|  136.36|Srta. Isadora Cor...|            77102442|              Nubank|  Francisco da Costa|             96088386|               Nubank|           cpf|    85907632429|2021-12-30 23:18:00|\n",
            "|  9|  574.39|   Dr. Lucas da Cruz|            38501170|                 BTG|       Calebe da Luz|             19365554|             Bradesco|           cpf|    64720189520|2021-06-21 07:20:00|\n",
            "| 10|   42.88|     Mirella Martins|            29535709|            Bradesco|        Danilo Lopes|             60064650|                 Itau|           cpf|    87014935232|2022-09-21 17:19:00|\n",
            "+---+--------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+--------------+---------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Porém, como saber se a manipulação de dados com Dataframes não é mais rápida que SQL?\n",
        "\n",
        "Para isso vamos propor um group by das duas maneiras e verificar qual é o plano de execução que o spark cria."
      ],
      "metadata": {
        "id": "XXcRV01OotXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "group_sql = spark.sql(\"select chave_pix_tipo, count(*) from base_pix group by chave_pix_tipo\")"
      ],
      "metadata": {
        "id": "GomfZBT9ot-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group_dataframe = df.groupBy('chave_pix_tipo').count()"
      ],
      "metadata": {
        "id": "YgWS4ogJo7KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"SQL Group\")\n",
        "group_sql.explain()\n",
        "\n",
        "print(\"DataFrame Group\")\n",
        "group_dataframe.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qksol06p6Z8",
        "outputId": "a0c0753a-7a87-4c04-b9f5-dfbe2623e2b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SQL Group\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[chave_pix_tipo#74], functions=[count(1)])\n",
            "   +- Exchange hashpartitioning(chave_pix_tipo#74, 200), ENSURE_REQUIREMENTS, [plan_id=41]\n",
            "      +- HashAggregate(keys=[chave_pix_tipo#74], functions=[partial_count(1)])\n",
            "         +- FileScan csv [chave_pix_tipo#74] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/base_de_dados.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<chave_pix_tipo:string>\n",
            "\n",
            "\n",
            "DataFrame Group\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[chave_pix_tipo#52], functions=[count(1)])\n",
            "   +- Exchange hashpartitioning(chave_pix_tipo#52, 200), ENSURE_REQUIREMENTS, [plan_id=54]\n",
            "      +- HashAggregate(keys=[chave_pix_tipo#52], functions=[partial_count(1)])\n",
            "         +- FileScan csv [chave_pix_tipo#52] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/base_de_dados.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<chave_pix_tipo:string>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "  \"\"\"\n",
        "    select chave_pix_tipo, sum(valor)\n",
        "    from base_pix\n",
        "    group by 1\n",
        "  \"\"\"\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olwgJtWQp6TS",
        "outputId": "0af6840f-6764-47af-a949-5b457528d330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------------------+\n",
            "|chave_pix_tipo|        sum(valor)|\n",
            "+--------------+------------------+\n",
            "|       celular|         207778.46|\n",
            "|         email|499009.38000000006|\n",
            "|           cpf| 659513.3499999997|\n",
            "+--------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "  \"\"\"\n",
        "    select chave_pix_tipo, round(sum(valor), 2)\n",
        "    from base_pix\n",
        "    group by 1\n",
        "  \"\"\"\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKQ7_wNGp6M6",
        "outputId": "c3e5e682-d426-45ec-9662-49402fd70073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------------+\n",
            "|chave_pix_tipo|round(sum(valor), 2)|\n",
            "+--------------+--------------------+\n",
            "|       celular|           207778.46|\n",
            "|         email|           499009.38|\n",
            "|           cpf|           659513.35|\n",
            "+--------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "  \"\"\"\n",
        "    select chave_pix_tipo, round(sum(valor), 2) as sum_valor\n",
        "    from base_pix\n",
        "    group by 1\n",
        "  \"\"\"\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CW6Y8Y6NqdJu",
        "outputId": "78b0a801-a0e6-4287-f926-eaa68aefdf7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+---------+\n",
            "|chave_pix_tipo|sum_valor|\n",
            "+--------------+---------+\n",
            "|       celular|207778.46|\n",
            "|         email|499009.38|\n",
            "|           cpf|659513.35|\n",
            "+--------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "  \"\"\"\n",
        "    select chave_pix_tipo, count(*) as count\n",
        "    from base_pix\n",
        "    group by 1\n",
        "  \"\"\"\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGex2D9zqfm4",
        "outputId": "06c1f70c-7c7b-4817-bebe-f1b3c2f1e74e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----+\n",
            "|chave_pix_tipo|count|\n",
            "+--------------+-----+\n",
            "|       celular|   22|\n",
            "|         email|   29|\n",
            "|           cpf|   49|\n",
            "+--------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "    \"\"\"\n",
        "      select parte_creditada_banco, valor,\n",
        "        row_number() over (partition by parte_creditada_banco order by valor desc) as row_number\n",
        "      from transacoes_pix\n",
        "      limit 10\n",
        "    \"\"\"\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "moAV5ZecrEW-",
        "outputId": "d67f9b36-57f9-4884-c69d-b5f519680bf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-2143605d9fc8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m spark.sql(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"\n\u001b[1;32m      3\u001b[0m       \u001b[0mselect\u001b[0m \u001b[0mparte_creditada_banco\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mrow_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mover\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m \u001b[0mby\u001b[0m \u001b[0mparte_creditada_banco\u001b[0m \u001b[0morder\u001b[0m \u001b[0mby\u001b[0m \u001b[0mvalor\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrow_number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0;32mfrom\u001b[0m \u001b[0mtransacoes_pix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m             \u001b[0mlitArgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1440\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitArgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1441\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `transacoes_pix` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 4 pos 11;\n'GlobalLimit 10\n+- 'LocalLimit 10\n   +- 'Project ['parte_creditada_banco, 'valor, row_number() windowspecdefinition('parte_creditada_banco, 'valor DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_number#390]\n      +- 'UnresolvedRelation [transacoes_pix], [], false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "  \"\"\"\n",
        "  with base_pix_row_number as(\n",
        "    select\n",
        "      parte_creditada_banco,\n",
        "      data_transacao,\n",
        "      row_number() over (partition by parte_creditada_banco order by data_transacao desc) as row_number\n",
        "    from base_pix\n",
        "  ) select\n",
        "      parte_creditada_banco,\n",
        "      data_transacao\n",
        "    from base_pix_row_number\n",
        "    where row_number = 1\n",
        "    order by data_transacao desc\n",
        "  \"\"\"\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rygAbRLIrEUH",
        "outputId": "00d36ae4-0edb-4e54-a07f-765769dbe34e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+-------------------+\n",
            "|parte_creditada_banco|     data_transacao|\n",
            "+---------------------+-------------------+\n",
            "|                 Itau|2022-12-15 01:29:00|\n",
            "|                  BTG|2022-12-08 23:47:00|\n",
            "|               Nubank|2022-11-19 19:25:00|\n",
            "|             Bradesco|2022-08-07 17:01:00|\n",
            "+---------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Porém, não precisa ficar limitado somente a execução de queries SQL.\n",
        "\n",
        "Podemos pegar o resultado de uma query e retorná-la para um DataFrame!"
      ],
      "metadata": {
        "id": "jtO_Ph60rQ2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_window = spark.sql(\n",
        "  \"\"\"\n",
        "  with base_pix_row_number as(\n",
        "    select\n",
        "      parte_creditada_banco,\n",
        "      data_transacao,\n",
        "      row_number() over (partition by parte_creditada_banco order by data_transacao desc) as row_number\n",
        "    from base_pix\n",
        "  ) select\n",
        "      parte_creditada_banco,\n",
        "      data_transacao\n",
        "    from base_pix_row_number\n",
        "    where row_number = 1\n",
        "    order by data_transacao desc\n",
        "  \"\"\"\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKTV1umGrERO",
        "outputId": "ed1757ef-7c02-42ee-82cb-d83e38812fe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+-------------------+\n",
            "|parte_creditada_banco|     data_transacao|\n",
            "+---------------------+-------------------+\n",
            "|                 Itau|2022-12-15 01:29:00|\n",
            "|                  BTG|2022-12-08 23:47:00|\n",
            "|               Nubank|2022-11-19 19:25:00|\n",
            "|             Bradesco|2022-08-07 17:01:00|\n",
            "+---------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "id": "qwH0cQmErYg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.selectExpr(\n",
        "    \"date(data_transacao) as date_data_transacao\",\n",
        "    \"va\"\n",
        ").groupBy('date_data_transacao').count().orderBy(col('count').desc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "kiMju8ZDraIQ",
        "outputId": "98b9480f-5978-4f15-d3b0-4ac9ec617b76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-aa5ae4a42e6a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m df.selectExpr(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"date(data_transacao) as date_data_transacao\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"va\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ).groupBy('date_data_transacao').count().orderBy(col('count').desc()).show()\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselectExpr\u001b[0;34m(self, *expr)\u001b[0m\n\u001b[1;32m   3074\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m             \u001b[0mexpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3076\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3077\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `va` cannot be resolved. Did you mean one of the following? [`id`, `valor`, `chave_pix_tipo`, `chave_pix_valor`, `data_transacao`].; line 1 pos 0;\n'Project [cast(data_transacao#54 as date) AS date_data_transacao#355, 'va]\n+- Relation [id#44,valor#45,parte_debitada_nome#46,parte_debitada_conta#47,parte_debitada_banco#48,parte_creditada_nome#49,parte_creditada_conta#50,parte_creditada_banco#51,chave_pix_tipo#52,chave_pix_valor#53,data_transacao#54] csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercício\n",
        "1. Vimos que há dois dias em que houve duas transações pix. Descubra são os ids dessas transações.\n",
        "\n",
        "2. Vimos que há dois dias em que houve duas transações pix. Descubra quais chaves pix foram utilizadas para realizar as transações."
      ],
      "metadata": {
        "id": "ZLuExCpzreJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lista_datas = spark.sql(\n",
        "  \"\"\"\n",
        "  select\n",
        "    date(data_transacao) as date_data_transacao\n",
        "  from base_pix\n",
        "  group by 1\n",
        "  having count(*) > 1\n",
        "  \"\"\"\n",
        ").collect()[0][0]\n",
        "lista_datas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66com7nqraBX",
        "outputId": "65f889d1-8b7c-41f8-a2f6-48e271d508e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.date(2022, 2, 26)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}